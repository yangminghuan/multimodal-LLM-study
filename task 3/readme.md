## 任务3：CLIP介绍
- 任务说明：理解CLIP模型原理，并完成的CLIP的加载
- 任务要求：
    - 理解CLIP模型的训练过程
    - 理解CLIP模型的预测方法

CLIP（Contrastive Language-Image Pre-Training）模型是由OpenAI在2021年发布的一种多模态预训练神经网络。它通过从自然语言监督中学习，能够有效地理解和连接图像和文本信息。CLIP的核心思想是使用大量的图像和文本配对数据进行预训练，从而学习到图像和文本之间的对齐关系，使得模型能够理解图像内容并将其与文本描述相关联。

CLIP模型包含两个主要部分：图像编码器（Image Encoder）和文本编码器（Text Encoder）。

1. 图像编码器（Image Encoder）：
    - 负责将输入的图像转换为低维向量表示（Embedding）。
    - 可以使用不同的架构，如ResNet或Vision Transformer（ViT）。
    - 在ViT架构中，图像首先被分割成多个小块（patches），然后加入位置信息，输入到Transformer模型中进行特征提取。
2. 文本编码器（Text Encoder）：
    - 将文本转换为低维向量表示。
    - 通常基于Transformer架构，执行对文本的小字节对编码（BPE）的表示。
    - 文本序列用特定的起始和结束标记括起来，以形成有效的输入序列。

CLIP模型的训练基于对比学习的原则，通过最大化正面图像-文本对的相似度并最小化负面对的相似度来进行。具体来说，模型被训练来预测哪些文本描述与给定的图像相匹配。在训练过程中，模型会接触到大量的图像和文本对，并学习将它们映射到共同的嵌入空间中，以便能够准确地进行匹配。

CLIP模型因其强大的多模态理解能力，在多个领域展现出了卓越的性能，包括但不限于：
- 图文匹配：确定给定的文本描述是否与图像内容相匹配。
- 图像检索：根据文本查询找到相关的图像。
- 零样本学习：在没有看过特定类别的样本的情况下，对图像进行分类。
